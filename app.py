import time
import logging
import requests
import feedparser
import threading
import sqlite3
import hashlib
from datetime import datetime, timezone, timedelta
from typing import Any
from abc import ABC, abstractmethod
from pydantic import BaseModel
from bs4 import BeautifulSoup
from apscheduler.schedulers.blocking import BlockingScheduler

# å®šæ•°
REQUEST_TIMEOUT = 30
MAX_ARTICLES_PER_SITE = 10
DATABASE_PATH = "news_notify_app.db"
TRANSLATION_API_URL = "https://api.mymemory.translated.net/get"

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def translate_to_japanese(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
    if not text or not text.strip():
        return text

    # æ—¢ã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    if any(
        "\u3040" <= char <= "\u309f"
        or "\u30a0" <= char <= "\u30ff"
        or "\u4e00" <= char <= "\u9faf"
        for char in text
    ):
        logger.debug(f"æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ç¿»è¨³ã‚’ã‚¹ã‚­ãƒƒãƒ—: {text[:50]}...")
        return text

    try:
        params = {
            "q": text,
            "langpair": "en|ja",
            "de": "your-email@example.com",  # MyMemory APIã§ã¯ä»»æ„ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŒ‡å®š
        }

        response = requests.get(
            TRANSLATION_API_URL, params=params, timeout=REQUEST_TIMEOUT
        )
        response.raise_for_status()

        data = response.json()

        if data.get("responseStatus") == 200:
            translated_text = data.get("responseData", {}).get("translatedText", text)
            logger.info(f"ç¿»è¨³æˆåŠŸ: {text[:30]}... â†’ {translated_text[:30]}...")
            return translated_text
        else:
            logger.warning(
                f"ç¿»è¨³APIå¿œç­”ã‚¨ãƒ©ãƒ¼: {data.get('responseDetails', 'Unknown error')}"
            )
            return text

    except requests.RequestException as e:
        logger.error(f"ç¿»è¨³APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return text
    except Exception as e:
        logger.error(f"ç¿»è¨³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return text


class Article(BaseModel):
    """è¨˜äº‹ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    title: str
    url: str
    original_title: str | None = None  # ç¿»è¨³å‰ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¿ã‚¤ãƒˆãƒ«

    def to_embed_dict(self) -> dict[str, str]:
        """DiscordåŸ‹ã‚è¾¼ã¿ç”¨ã®è¾æ›¸ã«å¤‰æ›"""
        return {"title": self.title, "url": self.url}

    def get_hash(self) -> str:
        """è¨˜äº‹ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
        # ãƒãƒƒã‚·ãƒ¥å€¤ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¿ã‚¤ãƒˆãƒ«ã§ç”Ÿæˆï¼ˆç¿»è¨³ã«ã‚ˆã‚‹é‡è¤‡ã‚’é˜²ãï¼‰
        original_title = self.original_title or self.title
        content = f"{original_title}|{self.url}"
        return hashlib.md5(content.encode("utf-8")).hexdigest()

    def translate_title(self) -> "Article":
        """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸæ–°ã—ã„Articleã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™"""
        if not self.original_title:
            # åˆå›ç¿»è¨³ã®å ´åˆã€ç¾åœ¨ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚ªãƒªã‚¸ãƒŠãƒ«ã¨ã—ã¦ä¿å­˜
            translated_title = translate_to_japanese(self.title)
            return Article(
                title=translated_title, url=self.url, original_title=self.title
            )
        else:
            # æ—¢ã«ç¿»è¨³æ¸ˆã¿ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            return self


class Webhook(BaseModel):
    """Webhookã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    id: int | None = None
    name: str
    endpoint: str
    service_type: str  # discord, slack, teams, etc.
    is_active: bool = True
    created_at: str | None = None


class NotificationService(ABC):
    """é€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, webhook: Webhook):
        self.webhook = webhook

    @abstractmethod
    def create_payload(
        self, website: "Website", articles: list[Article]
    ) -> dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ä½œæˆ"""
        pass

    @abstractmethod
    def get_headers(self) -> dict[str, str]:
        """ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        pass

    def send_notification(self, website: "Website", articles: list[Article]) -> bool:
        """é€šçŸ¥ã‚’é€ä¿¡"""
        if not articles:
            logger.info(
                f"æŠ•ç¨¿ã™ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“: {website.name} -> {self.webhook.name}"
            )
            return True

        payload = self.create_payload(website, articles)
        headers = self.get_headers()
        max_retries = 3

        for attempt in range(1, max_retries + 1):
            try:
                response = requests.post(
                    self.webhook.endpoint,
                    json=payload,
                    headers=headers,
                    timeout=REQUEST_TIMEOUT,
                )
                response.raise_for_status()

                logger.info(
                    f"{self.webhook.service_type}æŠ•ç¨¿æˆåŠŸ: {website.name} -> {self.webhook.name} ({len(articles)}ä»¶)"
                )
                return True

            except requests.RequestException as e:
                logger.error(
                    f"{self.webhook.service_type}æŠ•ç¨¿ã‚¨ãƒ©ãƒ¼ [{website.name} -> {self.webhook.name}] (è©¦è¡Œ {attempt}/{max_retries}): {e}"
                )
                if attempt < max_retries:
                    time.sleep(1)
                else:
                    return False
            except Exception as e:
                logger.error(
                    f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ [{website.name} -> {self.webhook.name}]: {e}"
                )
                return False


class DiscordService(NotificationService):
    """Discordé€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹"""

    def create_payload(
        self, website: "Website", articles: list[Article]
    ) -> dict[str, Any]:
        """Discordç”¨ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ä½œæˆ"""
        embeds = [article.to_embed_dict() for article in articles]

        return {
            "username": website.name,
            "avatar_url": website.avatar,
            "content": f"*æ–°ç€ãƒ‹ãƒ¥ãƒ¼ã‚¹* ({len(articles)}ä»¶)",
            "embeds": embeds,
        }

    def get_headers(self) -> dict[str, str]:
        """Discordç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        return {"Content-Type": "application/json"}


class SlackService(NotificationService):
    """Slacké€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹"""

    def create_payload(
        self, website: "Website", articles: list[Article]
    ) -> dict[str, Any]:
        """Slackç”¨ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ä½œæˆ"""
        blocks = []

        # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
        blocks.append(
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": f"ğŸ“° {website.name} - æ–°ç€ãƒ‹ãƒ¥ãƒ¼ã‚¹ ({len(articles)}ä»¶)",
                },
            }
        )

        # è¨˜äº‹ãƒªã‚¹ãƒˆ
        for article in articles:
            blocks.append(
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"â€¢ <{article.url}|{article.title}>",
                    },
                }
            )

        return {"username": website.name, "icon_url": website.avatar, "blocks": blocks}

    def get_headers(self) -> dict[str, str]:
        """Slackç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        return {"Content-Type": "application/json"}


class TeamsService(NotificationService):
    """Microsoft Teamsé€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹"""

    def create_payload(
        self, website: "Website", articles: list[Article]
    ) -> dict[str, Any]:
        """Teamsç”¨ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ä½œæˆï¼ˆAdaptive Cardså½¢å¼ï¼‰"""
        content_body = [
            {
                "type": "TextBlock",
                "text": f"{website.name} - æ–°ç€ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "weight": "Bolder",
                "size": "Medium",
                "wrap": True,
            }
        ]

        for article in articles:
            content_body.append(
                {
                    "type": "TextBlock",
                    "text": f"- [{article.title}]({article.url})",
                    "wrap": True,
                    "markdown": True,
                }
            )

        return {
            "attachments": [
                {
                    "contentType": "application/vnd.microsoft.card.adaptive",
                    "content": {
                        "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
                        "type": "AdaptiveCard",
                        "version": "1.2",
                        "body": content_body,
                    },
                }
            ]
        }

    def get_headers(self) -> dict[str, str]:
        """Teamsç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        return {"Content-Type": "application/json"}


class Website(BaseModel):
    """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    id: int | None = None
    name: str
    type: str
    url: str
    avatar: str | None = None
    selector: str | None = None  # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”¨ã‚»ãƒ¬ã‚¯ã‚¿
    is_active: bool = True
    needs_translation: bool = False  # ç¿»è¨³ãŒå¿…è¦ã‹ã®ãƒ•ãƒ©ã‚°,
    target_webhook_ids: str | None = None
    created_at: str | None = None

    def fetch_articles(self) -> list[Article]:
        """è¨˜äº‹ã‚’å–å¾—ã™ã‚‹æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰"""
        raise NotImplementedError("Subclasses must implement fetch_articles()")

    def _validate_url(self, url: str) -> str:
        """URLã®æ¤œè¨¼ã¨æ­£è¦åŒ–"""
        if not url.startswith(("http://", "https://")):
            if self.url.endswith("/") and not url.startswith("/"):
                return f"{self.url}{url}"
            elif not self.url.endswith("/") and url.startswith("/"):
                return f"{self.url}{url}"
            else:
                return f"{self.url}/{url}"
        return url


class ArticleDatabase:
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, db_path: str = DATABASE_PATH):
        self.db_path = db_path
        self._init_database()

    def _init_database(self) -> None:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆæœŸåŒ–"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # è¨˜äº‹ãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute(
                    """
                    CREATE TABLE IF NOT EXISTS articles (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        hash TEXT UNIQUE NOT NULL,
                        title TEXT NOT NULL,
                        url TEXT NOT NULL,
                        site_name TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """
                )

                # Webhookãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute(
                    """
                    CREATE TABLE IF NOT EXISTS webhooks (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        name TEXT UNIQUE NOT NULL,
                        endpoint TEXT NOT NULL,
                        service_type TEXT NOT NULL,
                        is_active BOOLEAN DEFAULT 1,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """
                )

                # Websiteãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute(
                    """
                    CREATE TABLE IF NOT EXISTS websites (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        name TEXT UNIQUE NOT NULL,
                        type TEXT NOT NULL,
                        url TEXT NOT NULL,
                        avatar TEXT,
                        selector TEXT,
                        is_active BOOLEAN DEFAULT 1,
                        needs_translation BOOLEAN DEFAULT 0,
                        target_webhook_ids TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """
                )

                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                cursor.execute(
                    """
                    CREATE INDEX IF NOT EXISTS idx_hash ON articles(hash)
                """
                )
                cursor.execute(
                    """
                    CREATE INDEX IF NOT EXISTS idx_site_created ON articles(site_name, created_at)
                """
                )
                cursor.execute(
                    """
                    CREATE INDEX IF NOT EXISTS idx_webhook_active ON webhooks(is_active)
                """
                )
                cursor.execute(
                    """
                    CREATE INDEX IF NOT EXISTS idx_website_active ON websites(is_active)
                """
                )
                cursor.execute(
                    """
                    CREATE INDEX IF NOT EXISTS idx_website_type ON websites(type)
                """
                )

                conn.commit()
                logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")
        except sqlite3.Error as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def is_article_exists(self, article: Article) -> bool:
        """è¨˜äº‹ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT 1 FROM articles WHERE hash = ? LIMIT 1",
                    (article.get_hash(),),
                )
                return cursor.fetchone() is not None
        except sqlite3.Error as e:
            logger.error(f"è¨˜äº‹å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def save_article(self, article: Article, site_name: str) -> bool:
        """è¨˜äº‹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    INSERT OR IGNORE INTO articles (hash, title, url, site_name)
                    VALUES (?, ?, ?, ?)
                """,
                    (article.get_hash(), article.title, article.url, site_name),
                )
                conn.commit()
                return cursor.rowcount > 0
        except sqlite3.Error as e:
            logger.error(f"è¨˜äº‹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def save_articles(self, articles: list[Article], site_name: str) -> int:
        """è¤‡æ•°ã®è¨˜äº‹ã‚’ä¸€æ‹¬ä¿å­˜"""
        saved_count = 0
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                for article in articles:
                    cursor.execute(
                        """
                        INSERT OR IGNORE INTO articles (hash, title, url, site_name)
                        VALUES (?, ?, ?, ?)
                    """,
                        (article.get_hash(), article.title, article.url, site_name),
                    )
                    if cursor.rowcount > 0:
                        saved_count += 1
                conn.commit()
        except sqlite3.Error as e:
            logger.error(f"è¨˜äº‹ä¸€æ‹¬ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return saved_count

    def filter_new_articles(self, articles: list[Article]) -> list[Article]:
        """æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        new_articles = []
        for article in articles:
            if not self.is_article_exists(article):
                new_articles.append(article)
        return new_articles

    def get_article_count(self, site_name: str | None = None) -> int:
        """è¨˜äº‹æ•°ã‚’å–å¾—"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                if site_name:
                    cursor.execute(
                        "SELECT COUNT(*) FROM articles WHERE site_name = ?",
                        (site_name,),
                    )
                else:
                    cursor.execute("SELECT COUNT(*) FROM articles")
                return cursor.fetchone()[0]
        except sqlite3.Error as e:
            logger.error(f"è¨˜äº‹æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    def cleanup_old_articles(self, days: int = 30) -> int:
        """å¤ã„è¨˜äº‹ã‚’å‰Šé™¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30æ—¥ä»¥ä¸Šå‰ï¼‰"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    DELETE FROM articles
                    WHERE created_at < datetime('now', '-{} days')
                """.format(
                        days
                    )
                )
                conn.commit()
                deleted_count = cursor.rowcount
                logger.info(f"å¤ã„è¨˜äº‹ã‚’å‰Šé™¤: {deleted_count}ä»¶")
                return deleted_count
        except sqlite3.Error as e:
            logger.error(f"å¤ã„è¨˜äº‹å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    # Webhookç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰
    def add_webhook(self, webhook: Webhook) -> bool:
        """Webhookã‚’è¿½åŠ """
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    INSERT INTO webhooks (name, endpoint, service_type, is_active)
                    VALUES (?, ?, ?, ?)
                """,
                    (
                        webhook.name,
                        webhook.endpoint,
                        webhook.service_type,
                        webhook.is_active,
                    ),
                )
                conn.commit()
                logger.info(f"Webhookè¿½åŠ : {webhook.name} ({webhook.service_type})")
                return True
        except sqlite3.IntegrityError:
            logger.error(f"WebhookåãŒé‡è¤‡ã—ã¦ã„ã¾ã™: {webhook.name}")
            return False
        except sqlite3.Error as e:
            logger.error(f"Webhookè¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def get_active_webhooks(self) -> list[Webhook]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªWebhookã‚’å–å¾—"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT id, name, endpoint, service_type, is_active, created_at
                    FROM webhooks
                    WHERE is_active = 1
                    ORDER BY created_at
                """
                )

                webhooks = []
                for row in cursor.fetchall():
                    webhook = Webhook(
                        id=row[0],
                        name=row[1],
                        endpoint=row[2],
                        service_type=row[3],
                        is_active=bool(row[4]),
                        created_at=row[5],
                    )
                    webhooks.append(webhook)

                return webhooks
        except sqlite3.Error as e:
            logger.error(f"Webhookå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def update_webhook_status(self, webhook_id: int, is_active: bool) -> bool:
        """Webhookã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ã‚’æ›´æ–°"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    UPDATE webhooks SET is_active = ? WHERE id = ?
                """,
                    (is_active, webhook_id),
                )
                conn.commit()
                return cursor.rowcount > 0
        except sqlite3.Error as e:
            logger.error(f"WebhookçŠ¶æ…‹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def delete_webhook(self, webhook_id: int) -> bool:
        """Webhookã‚’å‰Šé™¤"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("DELETE FROM webhooks WHERE id = ?", (webhook_id,))
                conn.commit()
                return cursor.rowcount > 0
        except sqlite3.Error as e:
            logger.error(f"Webhookå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    # Websiteç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰
    def add_website(self, website: Website) -> bool:
        """Websiteã‚’è¿½åŠ """
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    INSERT INTO websites (name, type, url, avatar, selector, is_active, needs_translation, target_webhook_ids)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        website.name,
                        website.type,
                        website.url,
                        website.avatar,
                        website.selector,
                        website.is_active,
                        website.needs_translation,
                        website.target_webhook_ids,
                    ),
                )
                conn.commit()
                logger.info(f"Websiteè¿½åŠ : {website.name} ({website.type})")
                return True
        except sqlite3.IntegrityError:
            logger.error(f"WebsiteåãŒé‡è¤‡ã—ã¦ã„ã¾ã™: {website.name}")
            return False
        except sqlite3.Error as e:
            logger.error(f"Websiteè¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def get_active_websites(self) -> list[Website]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªWebsiteã‚’å–å¾—"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT id, name, type, url, avatar, selector, is_active, needs_translation, target_webhook_ids, created_at
                    FROM websites
                    WHERE is_active = 1
                    ORDER BY created_at
                """
                )

                websites = []
                for row in cursor.fetchall():
                    website = Website(
                        id=row[0],
                        name=row[1],
                        type=row[2],
                        url=row[3],
                        avatar=row[4],
                        selector=row[5],
                        is_active=bool(row[6]),
                        needs_translation=bool(row[7]),
                        target_webhook_ids=row[8],
                        created_at=row[9],
                    )
                    websites.append(website)

                return websites
        except sqlite3.Error as e:
            logger.error(f"Websiteå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def update_website_status(self, website_id: int, is_active: bool) -> bool:
        """Websiteã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ã‚’æ›´æ–°"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    UPDATE websites SET is_active = ? WHERE id = ?
                """,
                    (is_active, website_id),
                )
                conn.commit()
                return cursor.rowcount > 0
        except sqlite3.Error as e:
            logger.error(f"WebsiteçŠ¶æ…‹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def delete_website(self, website_id: int) -> bool:
        """Websiteã‚’å‰Šé™¤"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("DELETE FROM websites WHERE id = ?", (website_id,))
                conn.commit()
                return cursor.rowcount > 0
        except sqlite3.Error as e:
            logger.error(f"Websiteå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False


class RssSite(Website):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‚µã‚¤ãƒˆ"""

    def fetch_articles(self) -> list[Article]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
        try:
            logger.info(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—é–‹å§‹: {self.name}")
            feed = feedparser.parse(self.url)

            if feed.bozo:
                logger.warning(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®è§£æã«å•é¡ŒãŒã‚ã‚Šã¾ã™: {self.name}")

            articles = []
            for entry in feed.entries[:MAX_ARTICLES_PER_SITE]:
                if hasattr(entry, "title") and hasattr(entry, "link"):
                    title = entry.title.strip()
                    link = self._validate_url(entry.link)
                    if title and link:
                        articles.append(Article(title=title, url=link))

            logger.info(f"RSSè¨˜äº‹å–å¾—å®Œäº†: {self.name} ({len(articles)}ä»¶)")
            return articles

        except Exception as e:
            logger.error(f"RSSè¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼ [{self.name}]: {e}")
            return []


class ScrapingSite(Website):
    """Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‚µã‚¤ãƒˆ"""

    def fetch_articles(self) -> list[Article]:
        """Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§è¨˜äº‹ã‚’å–å¾—"""
        try:
            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {self.name}")

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            response = requests.get(self.url, headers=headers, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            anchors = soup.select(self.selector or "")

            articles: list[Article] = []
            for anchor in anchors[:MAX_ARTICLES_PER_SITE]:
                href = anchor.get("href")
                if href:
                    title = anchor.get_text(strip=True)
                    if title:
                        url = self._validate_url(href)
                        articles.append(Article(title=title, url=url))

            logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {self.name} ({len(articles)}ä»¶)")
            return articles

        except requests.RequestException as e:
            logger.error(f"HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ [{self.name}]: {e}")
            return []
        except Exception as e:
            logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ [{self.name}]: {e}")
            return []


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
db = ArticleDatabase()


def create_website_instance(website: Website) -> Website:
    """Websiteãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Websiteã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
    if website.type == "rss":
        return RssSite(
            id=website.id,
            name=website.name,
            type=website.type,
            url=website.url,
            avatar=website.avatar,
            selector=website.selector,
            is_active=website.is_active,
            needs_translation=website.needs_translation,
            target_webhook_ids=website.target_webhook_ids,
            created_at=website.created_at,
        )
    elif website.type == "scraping":
        return ScrapingSite(
            id=website.id,
            name=website.name,
            type=website.type,
            url=website.url,
            avatar=website.avatar,
            selector=website.selector,
            is_active=website.is_active,
            needs_translation=website.needs_translation,
            target_webhook_ids=website.target_webhook_ids,
            created_at=website.created_at,
        )
    else:
        raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„Websiteã‚¿ã‚¤ãƒ—: {website.type}")


def create_notification_service(webhook: Webhook) -> NotificationService:
    """Webhookã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ"""
    service_map = {
        "discord": DiscordService,
        "slack": SlackService,
        "teams": TeamsService,
    }

    service_class = service_map.get(webhook.service_type.lower())
    if not service_class:
        raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚µãƒ¼ãƒ“ã‚¹ã‚¿ã‚¤ãƒ—: {webhook.service_type}")

    return service_class(webhook)


def _send_to_webhook(
    webhook: Webhook, website: "Website", articles: list[Article]
) -> bool:
    """å˜ä¸€ã®Webhookã«é€šçŸ¥ã‚’é€ä¿¡"""
    try:
        service = create_notification_service(webhook)
        return service.send_notification(website, articles)
    except ValueError as e:
        logger.error(f"ã‚µãƒ¼ãƒ“ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼ [{webhook.name}]: {e}")
        return False
    except Exception as e:
        logger.error(f"æŠ•ç¨¿ã‚¨ãƒ©ãƒ¼ [{webhook.name}]: {e}")
        return False


def _get_target_webhooks(webhooks: list[Webhook], website: "Website") -> list[Webhook]:
    """å¯¾è±¡ã¨ãªã‚‹Webhookãƒªã‚¹ãƒˆã‚’å–å¾—"""
    if not hasattr(website, "target_webhook_ids") or not website.target_webhook_ids:
        # target_webhook_ids ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€å…¨ã¦ã®WebhookãŒå¯¾è±¡
        return webhooks

    # target_webhook_ids ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€æŒ‡å®šã•ã‚ŒãŸIDã®Webhookã®ã¿
    target_ids = [
        id.strip() for id in website.target_webhook_ids.split(",") if id.strip()
    ]
    return [webhook for webhook in webhooks if str(webhook.id) in target_ids]


def post_message(website: "Website", articles: list[Article]) -> bool:
    """å…¨ã¦ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªWebhookã«è¨˜äº‹ã‚’æŠ•ç¨¿"""
    if not articles:
        logger.info(f"æŠ•ç¨¿ã™ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“: {website.name}")
        return True

    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªWebhookã‚’å–å¾—
    all_webhooks = db.get_active_webhooks()
    if not all_webhooks:
        logger.error("æŠ•ç¨¿å…ˆã®WebhookãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False

    # å¯¾è±¡ã¨ãªã‚‹Webhookã‚’çµã‚Šè¾¼ã¿
    target_webhooks = _get_target_webhooks(all_webhooks, website)
    if not target_webhooks:
        logger.warning(f"å¯¾è±¡ã¨ãªã‚‹WebhookãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {website.name}")
        return False

    # å„Webhookã«ä¸¦è¡Œã—ã¦é€šçŸ¥é€ä¿¡
    success_count = 0
    for webhook in target_webhooks:
        if _send_to_webhook(webhook, website, articles):
            success_count += 1

    # æŠ•ç¨¿æˆåŠŸå¾Œã€è¨˜äº‹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    if success_count > 0:
        saved_count = db.save_articles(articles, website.name)
        logger.info(
            f"æŠ•ç¨¿å®Œäº†: {website.name} ({success_count}/{len(target_webhooks)} WebhookæˆåŠŸ, {saved_count}ä»¶DBä¿å­˜)"
        )
        return True
    else:
        logger.error(f"å…¨ã¦ã®Webhookã§æŠ•ç¨¿ã«å¤±æ•—: {website.name}")
        return False


def get_news_website_list() -> list[Website]:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®ãƒªã‚¹ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—"""
    website_data_list = db.get_active_websites()

    if not website_data_list:
        logger.warning("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªWebsiteãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []

    websites: list[Website] = []
    for website_data in website_data_list:
        try:
            website_instance = create_website_instance(website_data)
            websites.append(website_instance)
        except ValueError as e:
            logger.error(f"Websiteä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            continue

    logger.info(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆæ•°: {len(websites)}")
    return websites


def process_site(site: "Website") -> bool:
    """ã‚µã‚¤ãƒˆã®è¨˜äº‹ã‚’å‡¦ç†ã—ã¦Discordã«æŠ•ç¨¿"""
    try:
        logger.info(f"ã‚µã‚¤ãƒˆå‡¦ç†é–‹å§‹: {site.name}")

        # è¨˜äº‹ã‚’å–å¾—
        all_articles = site.fetch_articles()
        if not all_articles:
            logger.info(f"å–å¾—è¨˜äº‹ãªã—: {site.name}")
            return True

        # æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        new_articles = db.filter_new_articles(all_articles)
        if not new_articles:
            logger.info(
                f"æ–°ç€è¨˜äº‹ãªã—: {site.name} (å–å¾—: {len(all_articles)}ä»¶, æ—¢å­˜: {len(all_articles)}ä»¶)"
            )
            return True

        logger.info(
            f"æ–°ç€è¨˜äº‹ç™ºè¦‹: {site.name} (å–å¾—: {len(all_articles)}ä»¶, æ–°ç€: {len(new_articles)}ä»¶)"
        )

        # ç¿»è¨³ãŒå¿…è¦ãªå ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç¿»è¨³
        if site.needs_translation:
            logger.info(f"è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç¿»è¨³ä¸­: {site.name}")
            translated_articles = []
            for article in new_articles:
                translated_article = article.translate_title()
                translated_articles.append(translated_article)
            new_articles = translated_articles

        # æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’æŠ•ç¨¿
        success = post_message(site, new_articles)
        if success:
            logger.info(f"ã‚µã‚¤ãƒˆå‡¦ç†å®Œäº†: {site.name} ({len(new_articles)}ä»¶æŠ•ç¨¿)")
        else:
            logger.error(f"ã‚µã‚¤ãƒˆå‡¦ç†å¤±æ•—: {site.name}")

        return success

    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆå‡¦ç†ä¸­ã®äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ [{site.name}]: {e}")
        return False


def initialize_default_webhooks() -> None:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Webhookã‚’åˆæœŸåŒ–"""
    webhooks = db.get_active_webhooks()
    if not webhooks:
        logger.info(
            "WebhookãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«Webhookã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
        )


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼šå…¨ã‚µã‚¤ãƒˆã®è¨˜äº‹ã‚’ä¸¦è¡Œå‡¦ç†ã§å–å¾—ãƒ»æŠ•ç¨¿"""
    try:
        logger.info("ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å‡¦ç†é–‹å§‹")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆWebhookã®åˆæœŸåŒ–
        initialize_default_webhooks()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
        total_articles = db.get_article_count()
        webhook_count = len(db.get_active_webhooks())
        logger.info(
            f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨˜äº‹æ•°: {total_articles}ä»¶, ã‚¢ã‚¯ãƒ†ã‚£ãƒ–Webhookæ•°: {webhook_count}ä»¶"
        )

        # å¤ã„è¨˜äº‹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ30æ—¥ä»¥ä¸Šå‰ã®è¨˜äº‹ã‚’å‰Šé™¤ï¼‰
        if total_articles > 1000:  # è¨˜äº‹æ•°ãŒå¤šã„å ´åˆã®ã¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            db.cleanup_old_articles(30)

        news_sites = get_news_website_list()
        if not news_sites:
            logger.warning("å‡¦ç†å¯¾è±¡ã®ã‚µã‚¤ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return

        threads = []
        results = {}

        def thread_wrapper(site: "Website"):
            """ã‚¹ãƒ¬ãƒƒãƒ‰ç”¨ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°"""
            results[site.name] = process_site(site)

        # å„ã‚µã‚¤ãƒˆã‚’ä¸¦è¡Œå‡¦ç†
        for site in news_sites:
            thread = threading.Thread(
                target=thread_wrapper, args=(site,), name=f"Thread-{site.name}"
            )
            thread.start()
            threads.append(thread)

        # å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã®å®Œäº†ã‚’å¾…æ©Ÿ
        for thread in threads:
            thread.join()

        # çµæœã®é›†è¨ˆ
        successful = sum(1 for success in results.values() if success)
        total = len(results)

        # ã‚µã‚¤ãƒˆåˆ¥ã®è¨˜äº‹æ•°çµ±è¨ˆ
        for site in news_sites:
            site_count = db.get_article_count(site.name)
            logger.info(f"[{site.name}] ç™»éŒ²è¨˜äº‹æ•°: {site_count}ä»¶")

        logger.info(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å‡¦ç†å®Œäº†: {successful}/{total} ã‚µã‚¤ãƒˆæˆåŠŸ")

    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def run_scheduler() -> None:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œ"""
    try:
        logger.info("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹")
        scheduler = BlockingScheduler()
        # æ—¥æœ¬æ™‚é–“ï¼ˆUTC+9ï¼‰ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
        jst = timezone(timedelta(hours=9))

        scheduler.add_job(
            main,
            "cron",
            hour=9,
            minute=0,
            timezone=jst,
            id="news_collector",
            max_instances=1,  # åŒæ™‚å®Ÿè¡Œã‚’é˜²ã
        )
        scheduler.start()
    except KeyboardInterrupt:
        logger.info("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")


def run_once() -> None:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚’1å›ã ã‘å®Ÿè¡Œ"""
    logger.info("ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚’æ‰‹å‹•å®Ÿè¡Œã—ã¾ã™")
    try:
        main()
        logger.info("ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã®æ‰‹å‹•å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"æ‰‹å‹•å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    run_scheduler()
